{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe584c6-410e-4419-844e-d9b97524bff8",
   "metadata": {},
   "source": [
    "The Reinforcement Learning Algorithm implemented in this Assignment is PPO, Proximal Policy Optimization with CNN, Convolutional Neural Network Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65afba86-c545-4161-a4ab-7cb49294b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym_super_mario_bros==7.3.0\n",
      "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
      "     ------------------------------------- 198.6/198.6 KB 11.8 MB/s eta 0:00:00\n",
      "Collecting nes_py\n",
      "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
      "     ---------------------------------------- 76.7/76.7 KB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gym>=0.17.2\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
      "     ------------------------------------- 626.2/626.2 KB 38.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nes_py) (1.22.3)\n",
      "Collecting pyglet<=1.5.11,>=1.4.0\n",
      "  Downloading pyglet-1.5.11-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 71.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.48.2\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 KB ? eta 0:00:00\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.48.2->nes_py) (0.4.4)\n",
      "Using legacy 'setup.py install' for nes_py, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701355 sha256=3d438be2f7677d87f9755a3a1185f33ae9980c960103f3a5fb12eacd49120918\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\1a\\00\\fb\\fe5cf2860fb9b7bc860e28f00095a1f42c7b726dd6f42d1acc\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym-notices, tqdm, cloudpickle, gym, nes_py, gym_super_mario_bros\n",
      "  Running setup.py install for nes_py: started\n",
      "  Running setup.py install for nes_py: finished with status 'done'\n",
      "Successfully installed cloudpickle-2.0.0 gym-0.23.1 gym-notices-0.0.6 gym_super_mario_bros-7.3.0 nes_py-8.1.8 pyglet-1.5.11 tqdm-4.64.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3[extra]\n",
      "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
      "     ------------------------------------- 177.7/177.7 KB 11.2 MB/s eta 0:00:00\n",
      "Collecting torch>=1.8.1\n",
      "  Downloading torch-1.11.0-cp310-cp310-win_amd64.whl (158.0 MB)\n",
      "     ------------------------------------- 158.0/158.0 MB 13.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (1.22.3)\n",
      "Collecting gym==0.21\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 32.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (3.5.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     --------------------------------------- 10.6/10.6 MB 43.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (9.1.1)\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 46.3 MB/s eta 0:00:00\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-win_amd64.whl (35.4 MB)\n",
      "     --------------------------------------- 35.4/35.4 MB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Collecting ale-py~=0.7.4\n",
      "  Downloading ale_py-0.7.5-cp310-cp310-win_amd64.whl (935 kB)\n",
      "     ------------------------------------- 935.2/935.2 KB 29.8 MB/s eta 0:00:00\n",
      "Collecting autorom[accept-rom-license]~=0.4.2\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.7.1-py3-none-any.whl (28 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 KB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 126.7/126.7 KB ? eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp310-cp310-win_amd64.whl (903 kB)\n",
      "     ------------------------------------- 903.8/903.8 KB 19.4 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (58.1.0)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "     -------------------------------------- 224.9/224.9 KB 6.9 MB/s eta 0:00:00\n",
      "Collecting wheel>=0.26\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.8/97.8 KB ? eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ------------------------------------- 781.3/781.3 KB 16.4 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 20.3 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "     -------------------------------------- 156.7/156.7 KB 9.2 MB/s eta 0:00:00\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
      "Requirement already satisfied: six in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.5.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\computing\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.4)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.5/151.5 KB ? eta 0:00:00\n",
      "Using legacy 'setup.py install' for gym, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml): started\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=446447 sha256=302ea9712421934141475f32070719d02c6f24553ced61265dcbeb5c2faf2064\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\77\\d9\\1c\\8e9cdcfe1e3a7480ea1fec05326d3fec295e5ca0e87163b522\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, wheel, werkzeug, typing-extensions, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opencv-python, oauthlib, markdown, importlib-resources, gym, grpcio, click, cachetools, absl-py, torch, requests-oauthlib, pandas, google-auth, AutoROM.accept-rom-license, autorom, ale-py, stable-baselines3, google-auth-oauthlib, tensorboard\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.23.1\n",
      "    Uninstalling gym-0.23.1:\n",
      "      Successfully uninstalled gym-0.23.1\n",
      "  Running setup.py install for gym: started\n",
      "  Running setup.py install for gym: finished with status 'done'\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 absl-py-1.0.0 ale-py-0.7.5 autorom-0.4.2 cachetools-5.1.0 click-8.1.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 grpcio-1.46.1 gym-0.21.0 importlib-resources-5.7.1 markdown-3.3.7 oauthlib-3.2.0 opencv-python-4.5.5.64 pandas-1.4.2 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 stable-baselines3-1.5.0 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.11.0 typing-extensions-4.2.0 werkzeug-2.1.2 wheel-0.37.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.10.1+cu113 (from versions: 1.11.0, 1.11.0+cu113)\n",
      "ERROR: No matching distribution found for torch==1.10.1+cu113\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_super_mario_bros==7.3.0 nes_py\n",
    "\n",
    "#install stable baselines AI library https://stable-baselines3.readthedocs.io/en/master/guide/install.html\n",
    "!pip install stable-baselines3[extra]\n",
    "#Install pytorch https://pytorch.org/get-started/locally/\n",
    "!pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473c0186-3843-4050-a65f-b146b8d80404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the game\n",
    "import gym_super_mario_bros\n",
    "#import the joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "#Import thje SIMPLIFIED controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48cac99-1cf4-46d7-a914-ef267041aeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sIMPLE ACTIONS is a libraary that simplifies the actions that the ai can take to make it less complicated\n",
    "SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c571e63b-4864-4de1-9466-d002d65c1539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Train the RL Model\n",
    "#Import os for file path management\n",
    "import os\n",
    "#import PPO for algos\n",
    "from stable_baselines3 import DQN\n",
    "#import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c59cf8-eba9-408b-b38d-c4b11b7be991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 256, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')# setup which emulatiuon version to use https://github.com/Kautenja/gym-super-mario-bros\n",
    "#env.action_space\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT)#SIMPLIFY THE NUMBER OF ACTIONS the ai can take from 256 to 7\n",
    "#env.action_space#tells the number of actions available\n",
    "\n",
    "#observation space shows what output the game returns,  i.e the number of pixles in a frame of the game\n",
    "env.observation_space.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30ed30f-9037-4112-9170-de7fbe8735e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to save models after x amount of steps\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):#check frequency, how often to save the model, save_path, where to save the model\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624f84db-20e0-4e37-857a-5b51327ce591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computing\\Desktop\\MSc-Robotics-and-Autonomous-Systems\\Advanced Machine Learning\\Assignment Task 2\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = '../../../train/'#CREATE THESE ON THE DESKTOP IF THE GIT REPO IS CLONES ON THE DESKTOP\n",
    "LOG_DIR = '../../../logs/'\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f692b553-accb-4712-9793-4329b9557acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup callback Saves the model every 10,thousand steps at the desired location\n",
    "callback = TrainAndLoggingCallback(check_freq=1000,save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bc5dcb-748d-47ce-9751-c1de15fceebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "#CREATING AN AI MODEL\n",
    "#Setup the MODEL: Machine learning/Deep Learning/ Reinforcement Learning, Ai model\n",
    "model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001,buffer_size=100, seed =10 )#using the CNN cLassifier, tensorboard allows us to see how the training is performing, learning rate determines how fast the ai is learning\n",
    "#n_steps is how many frames to wait per game before updating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e36ab2-1a2b-4245-801c-8bc4a6dda878",
   "metadata": {},
   "source": [
    "## Train the AI model, this is where it starts to learn ,  how many frames the AI sees for every single game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4423f-f85f-4011-867b-885b15831508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../../../logs/DQN_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53e+04 |\n",
      "|    ep_rew_mean      | 787      |\n",
      "|    exploration_rate | 0.418    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 934      |\n",
      "|    total_timesteps  | 61245    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 2811     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.1e+04  |\n",
      "|    ep_rew_mean      | 912      |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 3061     |\n",
      "|    total_timesteps  | 88253    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 9563     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.91e+03 |\n",
      "|    ep_rew_mean      | 1.09e+03 |\n",
      "|    exploration_rate | 0.0986   |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 26       |\n",
      "|    time_elapsed     | 3598     |\n",
      "|    total_timesteps  | 94889    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.0477   |\n",
      "|    n_updates        | 11222    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.16e+03 |\n",
      "|    ep_rew_mean      | 1.08e+03 |\n",
      "|    exploration_rate | 0.0637   |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 25       |\n",
      "|    time_elapsed     | 3897     |\n",
      "|    total_timesteps  | 98559    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.692    |\n",
      "|    n_updates        | 12139    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.68e+03 |\n",
      "|    ep_rew_mean      | 1.06e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 5085     |\n",
      "|    total_timesteps  | 113663   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 15915    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17e+03 |\n",
      "|    ep_rew_mean      | 1.04e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 5931     |\n",
      "|    total_timesteps  | 124137   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.206    |\n",
      "|    n_updates        | 18534    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56e+03 |\n",
      "|    ep_rew_mean      | 1.07e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 6225     |\n",
      "|    total_timesteps  | 127811   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.344    |\n",
      "|    n_updates        | 19452    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.06e+03 |\n",
      "|    ep_rew_mean      | 1.06e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 6383     |\n",
      "|    total_timesteps  | 129769   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.38     |\n",
      "|    n_updates        | 19942    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97e+03 |\n",
      "|    ep_rew_mean      | 1.06e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 7451     |\n",
      "|    total_timesteps  | 142755   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 23188    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63e+03 |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 7640     |\n",
      "|    total_timesteps  | 145055   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.329    |\n",
      "|    n_updates        | 23763    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.43e+03 |\n",
      "|    ep_rew_mean      | 1.06e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 8129     |\n",
      "|    total_timesteps  | 150978   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 25244    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.26e+03 |\n",
      "|    ep_rew_mean      | 1.08e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 8579     |\n",
      "|    total_timesteps  | 156393   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.724    |\n",
      "|    n_updates        | 26598    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.05e+03 |\n",
      "|    ep_rew_mean      | 1.07e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 8763     |\n",
      "|    total_timesteps  | 158635   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.0582   |\n",
      "|    n_updates        | 27158    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.86e+03 |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 8904     |\n",
      "|    total_timesteps  | 160303   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 27575    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.73e+03 |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9206     |\n",
      "|    total_timesteps  | 163959   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.911    |\n",
      "|    n_updates        | 28489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.6e+03  |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9391     |\n",
      "|    total_timesteps  | 166209   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.796    |\n",
      "|    n_updates        | 29052    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | 1.03e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9498     |\n",
      "|    total_timesteps  | 167539   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.27     |\n",
      "|    n_updates        | 29384    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+03 |\n",
      "|    ep_rew_mean      | 1.02e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9793     |\n",
      "|    total_timesteps  | 171232   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 30307    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | 1.01e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9914     |\n",
      "|    total_timesteps  | 172682   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 30670    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.2e+03  |\n",
      "|    ep_rew_mean      | 1.02e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 10168    |\n",
      "|    total_timesteps  | 175668   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 31416    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+03 |\n",
      "|    ep_rew_mean      | 1.01e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 10286    |\n",
      "|    total_timesteps  | 177088   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 31771    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.03e+03 |\n",
      "|    ep_rew_mean      | 994      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 10404    |\n",
      "|    total_timesteps  | 178476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.375    |\n",
      "|    n_updates        | 32118    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.96e+03 |\n",
      "|    ep_rew_mean      | 991      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 10573    |\n",
      "|    total_timesteps  | 180524   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 32630    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.91e+03 |\n",
      "|    ep_rew_mean      | 992      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 16       |\n",
      "|    time_elapsed     | 10816    |\n",
      "|    total_timesteps  | 183442   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 33360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.92e+03 |\n",
      "|    ep_rew_mean      | 1.01e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 16       |\n",
      "|    time_elapsed     | 11565    |\n",
      "|    total_timesteps  | 192463   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.23     |\n",
      "|    n_updates        | 35615    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.36e+03 |\n",
      "|    ep_rew_mean      | 1.01e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 16       |\n",
      "|    time_elapsed     | 11998    |\n",
      "|    total_timesteps  | 197649   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1e-06    |\n",
      "|    loss             | 0.932    |\n",
      "|    n_updates        | 36912    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.learn(total_timesteps=1000000,callback=callback) # can remove callback to prevent saving the data\n",
    "\n",
    "#LOSS should be going down and explained_variance should be going up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473d163-4b1e-464d-a2ea-e12a712469a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../../ppo_mario_testmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef5ccb56-796d-4a26-b0fb-18f3de68ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test out the model\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = PPO.load('../../../train/best_model_51000')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e81480bb-9f97-4deb-9272-6d15f468df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a77c1ce-251a-499d-b93a-40f2d814a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m      6\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[1;32m----> 7\u001b[0m     state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[1;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackedobs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nes_py\\nes_env.py:293\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[0;32m    295\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True: \n",
    "    \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229d847-6a91-4f31-bbaa-85d8d27a1d81",
   "metadata": {},
   "source": [
    "#Run this code in the logs/PPO_1 To see performance metrics via tensorboard if it is installed\n",
    "\n",
    "tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4a5465-667f-4c54-b07d-39ccd9d6033f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Close the program if it still persits\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "#Close the program if it still persits\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3fae1-588b-4a96-ac19-f81bb2b9fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the environment for AI training\n",
    "#import frame stacker Wrapper and Grayscaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation # allows us to capture multiple frame while playing the game\n",
    "#Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv #converts the game into grayscale to cut down information\n",
    "#Import Matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf59f9-1ebe-4953-b6bf-ecf0f1db3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. create the base environment\n",
    "the AI library i will use is the Stable Baselines \n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')# setup which emulatiuon version to use https://github.com/Kautenja/gym-super-mario-bros\n",
    "# 2. simplify the controls\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT)#SIMPLIFY THE NUMBER OF ACTIONS the ai can take from 256 to 7\n",
    "# 3. Grayscale\n",
    "env = GrayScaleObservation(env, keep_dim=True)# required to keep the last dimention which determins colour to do frame stacking\n",
    "# 4. Wrap inside dummy environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# 5. Stack the frames\n",
    "env = VecFrameStack(env, 4, channels_order='last') #by parsing the env to the frame stack we can specify how many frames to stack, last argument specifies where the rgb channel is, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6d9eb-1854-4f85-950a-443c29abbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state, reward, done, info = env.step([env.action_space.sample()])#parses a random action to the game, this has to be in an array because the environment is now vectorised\n",
    "plt.imshow(state[0])#show the frame in colored form before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7c1c4e-b481-46de-9132-935b991bb855",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166f8d5-0258-4484-a116-021890dfce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701aa66-9dda-4d08-b4f1-bb4f6cd1eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True: \n",
    "    \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
